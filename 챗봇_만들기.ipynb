{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "챗봇 만들기.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkfHlqppqxdXdXbEUcUyVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pariskimhj/AI_class/blob/master/%EC%B1%97%EB%B4%87_%EB%A7%8C%EB%93%A4%EA%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCERXwIJq0D8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_g = pd.read_csv('ChatbotData.csv')\n",
        "text_g = text_g.A.copy()\n",
        "text_g.head()\n",
        "text_g.shape"
      ],
      "metadata": {
        "id": "kSzva9R1rOAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('ChatbotData.csv')['label'].value_counts().plot(kind='bar')\n",
        "list(text_g)[:10]"
      ],
      "metadata": {
        "id": "I1rWN0LHr2ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_g = Tokenizer()\n",
        "tokenizer_g.fit_on_texts(list(text_g))\n",
        "vocab_size_g = len(tokenizer_g.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size_g)"
      ],
      "metadata": {
        "id": "Gzh70J9erXG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer_g.word_index)\n",
        "sequences_g = list()\n",
        "for line in text_g: # 줄바꿈 문자를 기준으로 문장 토큰화\n",
        "    encoded = tokenizer_g.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences_g.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences_g))\n",
        "print(sequences_g)\n",
        "max_len_g = max(len(l) for l in sequences_g) \n",
        "print('샘플의 최대 길이 : {}'.format(max_len_g))"
      ],
      "metadata": {
        "id": "QrVvv6zsr9gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_g = pad_sequences(sequences_g, maxlen=max_len_g, padding='pre')\n",
        "print(sequences_g)\n"
      ],
      "metadata": {
        "id": "v22uSDplsFb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_g = np.array(sequences_g)\n",
        "X_chatbot_g = sequences_g[:,:-1]\n",
        "y_chatbot_g = sequences_g[:, -1]\n",
        "print('X:', X_chatbot_g)\n",
        "print('y:', y_chatbot_g)\n",
        "print('전체 y의 개수:', len(y_chatbot_g))\n",
        "print('고유값 y의 개수:', np.unique(y_chatbot_g)[-1]+1)"
      ],
      "metadata": {
        "id": "DOtPI3KysS6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_one_g = to_categorical(y_chatbot_g, num_classes=vocab_size_g)\n",
        "print(\"one_hot_vector y:\", y_one_g)"
      ],
      "metadata": {
        "id": "alf6EoC7sXUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model design\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN, LSTM\n",
        "embedding_dim = 32\n",
        "hidden_units = 64\n",
        "\n",
        "model_g = Sequential()\n",
        "model_g.add(Embedding(vocab_size_g, embedding_dim))\n",
        "model_g.add(SimpleRNN(hidden_units))\n",
        "model_g.add(Dense(vocab_size_g, activation='softmax'))\n",
        "model_g.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_g.fit(X_chatbot_g, y_one_g, epochs=200, verbose=2)\n",
        "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = currentword\n",
        "    sentence = ''\n",
        "\n",
        "    # n번 반복\n",
        "    for  in range(n):\n",
        "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # 예측 단어를 문장에 저장\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n",
        "print(sentence_generation(model_g, tokenizer_g, '너는', 5))"
      ],
      "metadata": {
        "id": "CG6aPly1saNc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}