{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "챗봇만들기2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO0+9b5iHI4mG1xWwLHUnq3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pariskimhj/AI_class/blob/master/%EC%B1%97%EB%B4%87%EB%A7%8C%EB%93%A4%EA%B8%B02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8jMD-YQs1Iu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "text = pd.read_csv('ChatbotData.csv')\n",
        "text.head()\n",
        "print(text.label.value_counts())\n",
        "text.label.value_counts().plot(kind='bar')\n",
        "plt.show()\n",
        "text.loc[text.label==0,].head(3) # 일상다반사 (중립)\n",
        "text.loc[text.label==1,].head(3) # 이별(부정)\n",
        "text.loc[text.label==2,].head(3) # 사랑(긍정)\n",
        "list(text['A'])[:5]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(list(text['A']))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)\n",
        "print(tokenizer.word_index)\n",
        "sequences = list()\n",
        "for line in text['A']: # 줄바꿈 문자를 기준으로 문장 토큰화\n",
        "    encoded = tokenizer.texts_to_sequences([line])[0]\n",
        "    sequences.append(encoded)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))\n",
        "print(list(text['A'])[:5])\n",
        "print(sequences[:5])\n",
        "# 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
        "max_len = max(len(l) for l in sequences) \n",
        "print('샘플의 최대 길이 : {}'.format(max_len))\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences)\n",
        "sequences = np.array(sequences)\n",
        "X_chatbot = sequences\n",
        "y_chatbot = text.label\n",
        "# y confirmation\n",
        "unique_y = np.unique(y_chatbot)[-1] + 1\n",
        "print('전체 y의 개수:', len(y_chatbot))\n",
        "print('고유값 y의 개수:', unique_y)\n",
        "# 고유값 y각각의 원핫벡터 변경, 총 13399 자리수 맞춤\n",
        "y_one = to_categorical(y_chatbot, num_classes=unique_y)\n",
        "print(\"one_hot_vector y:\", y_one)\n",
        "embedding_dim = 16\n",
        "hidden_units = 16\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(SimpleRNN(hidden_units))\n",
        "model.add(Dense(unique_y, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_chatbot, y_one, epochs=10, validati\n",
        "embedding_dim = 64\n",
        "hidden_units = 64\n",
        "\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_size, embedding_dim))\n",
        "model_lstm.add(LSTM(hidden_units))\n",
        "model_lstm.add(Dense(unique_y, activation='softmax'))\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.fit(X_chatbot, y_one, epochs=10, verbose=1, validation_split=0.2)\n",
        "def sentiment_analysis(model, tokenizer, current_word): # 모델, 토크나이저, 현재 단어\n",
        "  # 현재 단어에 대한 정수 인코딩과 패딩\n",
        "  encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "  encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "  # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "  result = model.predict(encoded, verbose=0)\n",
        "  result = np.argmax(result, axis=1)\n",
        "  if result == [0]:\n",
        "    word_result = '감성분석: 중립'\n",
        "  elif result == [1]:\n",
        "    word_result = '감성분석: 긍정'\n",
        "  else:\n",
        "    word_result = '감성분석: 부정' \n",
        "  return word_result\n",
        "sentiment_analysis(model, tokenizer, '하루가 또 가네요')\n",
        "def sentence_generation(model, tokenizer, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = currentword\n",
        "    sentence = ''\n",
        "\n",
        "    # n번 반복\n",
        "    for  in range(n):\n",
        "        # 현재 단어에 대한 정수 인코딩과 패딩\n",
        "        encoded = tokenizer.texts_to_sequences([current_word])[0]\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre')\n",
        "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        for word, index in tokenizer.word_index.items(): \n",
        "            # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면 break\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        current_word = current_word + ' '  + word\n",
        "\n",
        "        # 예측 단어를 문장에 저장\n",
        "        sentence = sentence + ' ' + word\n",
        "\n",
        "    sentence = init_word + sentence\n",
        "    return sentence\n",
        "new_text = sentence_generation(model_lstm_ga, tokenizer_ga, '너는', 5)\n",
        "print('생성단어: {}'.format(new_text))\n",
        "print(sentiment_analysis(model_lstm_ga, tokenizer_ga, new_text))"
      ]
    }
  ]
}